{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mlainas/bubble3jh/anaconda3/envs/ppg/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] [--seed SEED] [--device DEVICE]\n",
      "                             [--ignore_wandb] [--seq_length SEQ_LENGTH]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE] [--min_max]\n",
      "                             [--benchmark BENCHMARK] [--train_fold TRAIN_FOLD]\n",
      "                             [--load_checkpoint] [--final_layers FINAL_LAYERS]\n",
      "                             [--n_block N_BLOCK]\n",
      "                             [--diffusion_time_steps DIFFUSION_TIME_STEPS]\n",
      "                             [--train_epochs TRAIN_EPOCHS] [--init_lr INIT_LR]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--init_bias INIT_BIAS] [--final_bias FINAL_BIAS]\n",
      "                             [--optim OPTIM]\n",
      "                             [--loss {normal_loss,group_average_loss}]\n",
      "                             [--t_scheduling {loss-second-moment,uniform,train-step}]\n",
      "                             [--T_max T_MAX] [--eta_min ETA_MIN]\n",
      "                             [--target_group {0,1,2,3,4}]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/home/bubble3jh/.local/share/jupyter/runtime/kernel-v2-27825733My6YW2UFtsT.json could match --final_layers, --final_bias\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mlainas/bubble3jh/anaconda3/envs/ppg/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from denoising_diffusion_pytorch.model import *\n",
    "from denoising_diffusion_pytorch.denoising_diffusion_pytorch_1d_guided import Unet1D, GaussianDiffusion1D, Trainer1D, Dataset1D\n",
    "import paths\n",
    "from utils import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from denoising_diffusion_pytorch.resample import create_named_schedule_sampler\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def main(args):\n",
    "    set_seed(args.seed)\n",
    "    device = torch.device(\"cuda\")\n",
    "    batch_size = args.train_batch_size\n",
    "    diffuse_time_step = args.diffusion_time_steps \n",
    "    epochs=args.train_epochs\n",
    "    # data = get_data(sampling_method='first_k',\n",
    "    #                                 num_samples=5,\n",
    "    #                                 data_root=paths.DATA_ROOT,\n",
    "    #                                 benchmark='bcg',\n",
    "    #                                 train_fold=args.train_fold)\n",
    "    with open(f\"/mlainas/ETRI_2023/sampling_results/sample_fold_{args.train_fold}_val_res_gal.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f) \n",
    "\n",
    "    schedule_sampler = create_named_schedule_sampler(\n",
    "        args.t_scheduling, diffuse_time_step=diffuse_time_step,total_epochs=epochs, init_bias=args.init_bias, final_bias=args.final_bias\n",
    "    ) \n",
    "    \n",
    "    model_path = f\"/mlainas/ETRI_2023/reg_model/fold_{args.train_fold}/{args.t_scheduling}_epoch_{epochs}_diffuse_{diffuse_time_step}_wd_{args.weight_decay}_eta_{args.eta_min}_lr_{args.init_lr}_nblock_{args.n_block}_{args.final_layers}-layer-clf\"\n",
    "\n",
    "    tr_dataset = Dataset1D(data['train']['ppg'], label=data['train']['spdp'], groups=data['train']['group_label'] ,normalize=True)\n",
    "    tr_dl = DataLoader(tr_dataset, batch_size = batch_size, shuffle = True, pin_memory = True, num_workers = 0)\n",
    "    tr_dl = cycle(tr_dl)\n",
    "    \n",
    "    regressor = ResNet1D(output_size=2, final_layers=args.final_layers, n_block=args.n_block).to(device)\n",
    "    optimizer = optim.Adam(regressor.parameters(), lr=args.init_lr, weight_decay=args.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max, eta_min=args.eta_min)\n",
    "\n",
    "    checkpoint = torch.load(model_path+\"_resnet_gal.pt\")\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    optim_state_dict = checkpoint['optimizer_state_dict']\n",
    "\n",
    "    regressor.load_state_dict(model_state_dict)\n",
    "    optimizer.load_state_dict(optim_state_dict)\n",
    "\n",
    "    # regressor.load_state_dict(weight)\n",
    "    diffusion = GaussianDiffusion1D(\n",
    "        model = regressor,\n",
    "        seq_length = 625,\n",
    "        timesteps = diffuse_time_step,\n",
    "        objective = 'pred_v'\n",
    "    ).to(device)\n",
    "    best_train_loss = float('inf');best_val_train_loss=float('inf');best_val_loss_sbp = float('inf');best_val_loss_dbp = float('inf');val_loss_sbp = 0;val_loss_dbp = 0\n",
    "    with tqdm(initial = 0, total = epochs) as pbar:\n",
    "        for i in range(epochs):\n",
    "            mae_sbp_lists={}; mae_dbp_lists={}; overall_mae_sbp_list=[]; overall_mae_dbp_list=[]\n",
    "            # Train step\n",
    "            batch, spdp, g = next(tr_dl)\n",
    "            batch = batch.to(device); spdp=spdp.to(device); g=g.to(device)\n",
    "            t, _ = schedule_sampler.sample(batch.size(0), device)\n",
    "            # batch = diffusion.q_sample(batch, t) 노이즈 추가 안함\n",
    "            optimizer.zero_grad()\n",
    "            out = regressor(batch, t, g)\n",
    "            # for normal loss\n",
    "            loss = F.mse_loss(out, spdp, reduction=\"none\")\n",
    "            # for group loss\n",
    "            group = same_to_group(g)\n",
    "            group_losses={}\n",
    "            for g_i in torch.unique(group):\n",
    "                indices = torch.where(group == g_i)[0]\n",
    "                model_output_group = out[indices]\n",
    "                ground_truth_group = spdp[indices]\n",
    "                g_loss = F.mse_loss(model_output_group, ground_truth_group, reduction=\"mean\")\n",
    "                if g_i.item() not in group_losses:\n",
    "                    group_losses[g_i.item()] = []\n",
    "                group_losses[g_i.item()].append(g_loss)\n",
    "            total_loss = 0\n",
    "            for _, val in group_losses.items(): \n",
    "                total_loss += val[0]\n",
    "            group_avg_loss = total_loss/len(group_losses.keys())\n",
    "\n",
    "            overall_mae_sbp_list , overall_mae_dbp_list , mae_sbp_lists , mae_dbp_lists  = calculate_batch_mae(out, spdp, tr_dataset, g, mae_sbp_lists, mae_dbp_lists, overall_mae_sbp_list, overall_mae_dbp_list) \n",
    "            overall_mae_sbp, overall_mae_dbp, _, _ = log_global_metrics(args, overall_mae_sbp_list , overall_mae_dbp_list , mae_sbp_lists , mae_dbp_lists, \"train\")\n",
    "\n",
    "            if args.t_scheduling == \"loss-second-moment\":\n",
    "                schedule_sampler.update_with_local_losses(t,loss)\n",
    "            elif args.t_scheduling == \"train-step\":\n",
    "                schedule_sampler.set_epoch(i+1)\n",
    "            loss = loss.mean()\n",
    "            t_mean = t.sum().item()/len(t)\n",
    "            if not args.ignore_wandb :\n",
    "                wandb.log({\"train_loss\": loss.item(), \"t_mean\": t_mean})\n",
    "            if args.loss == \"normal_loss\":\n",
    "                loss.backward()\n",
    "            elif args.loss == \"group_average_loss\":\n",
    "                group_avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ## COMMON --------------------------------------------------\n",
    "    parser = argparse.ArgumentParser(description=\"generate ppg with regressor guidance\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1000, help=\"random seed (default: 1000)\")\n",
    "    parser.add_argument(\"--device\", type=str, default='cuda')\n",
    "    parser.add_argument(\"--ignore_wandb\", action='store_true',\n",
    "        help = \"Stop using wandb (Default : False)\")\n",
    "\n",
    "    ## DATA ----------------------------------------------------\n",
    "    parser.add_argument(\"--seq_length\", type=int, default=625)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--min_max\", action='store_false',\n",
    "        help = \"Min-Max normalize data (Default : True)\")\n",
    "    parser.add_argument(\"--benchmark\", type=str, default='bcg')\n",
    "    parser.add_argument(\"--train_fold\", type=int, default=0)\n",
    "\n",
    "    ## Model ---------------------------------------------------\n",
    "    parser.add_argument(\"--load_checkpoint\", action='store_true',\n",
    "        help = \"Resume model training (Default : False)\")\n",
    "    parser.add_argument(\"--final_layers\", type=int, default=3)\n",
    "    parser.add_argument(\"--n_block\", type=int, default=8)\n",
    "    \n",
    "    ## Training ------------------------------------------------\n",
    "    parser.add_argument(\"--diffusion_time_steps\", type=int, default=2000)\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=2000)\n",
    "    parser.add_argument(\"--init_lr\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--init_bias\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--final_bias\", type=float, default=1)\n",
    "    parser.add_argument(\"--optim\", type=str, default='adam')\n",
    "    parser.add_argument(\"--loss\", type=str, default='normal_loss',  choices=[\"normal_loss\", \"group_average_loss\"])\n",
    "    parser.add_argument(\"--t_scheduling\", type=str, default=\"uniform\",  choices=[\"loss-second-moment\", \"uniform\", \"train-step\"])\n",
    "    parser.add_argument(\"--T_max\", type=int, default=2000)  \n",
    "    parser.add_argument(\"--eta_min\", type=float, default=0)  \n",
    "\n",
    "    ## Sampling ------------------------------------------------\n",
    "    parser.add_argument(\"--target_group\", type=int, default=1, choices=[0,1,2,3,4], \n",
    "                        help=\"0(hyp0) 1(normal) 2(perhyper) 3(hyper2) 4(crisis) (Default : 1 (normal))\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
